{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twin_Delayed_DDPG_Ant.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXu1r8qvSzWf",
        "colab_type": "text"
      },
      "source": [
        "# **Twin-Delayed DDPG**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRzQUhuUTc0J",
        "colab_type": "text"
      },
      "source": [
        "## **INSTALLING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAHMB0Ze8fU0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "c9bde154-5368-43f1-f7cd-2ea8e2d71d0c"
      },
      "source": [
        "# Module for physics simulation\n",
        "!pip install pybullet"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pybullet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/94/9fdf9b24dd1ebdf2b7920d97c124d2df56edd0bdc8dff942e5a585c0fd86/pybullet-2.5.6.tar.gz (77.4MB)\n",
            "\u001b[K     |████████████████████████████████| 77.4MB 50kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pybullet\n",
            "  Building wheel for pybullet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybullet: filename=pybullet-2.5.6-cp36-cp36m-linux_x86_64.whl size=88636101 sha256=1421b7833a246e6f13a3169a9585f8208f0c9c3b2a23048f302e8a8a6116fe1d\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/22/24/936718f593d621ad167815e1b54e69135191954a9f52024328\n",
            "Successfully built pybullet\n",
            "Installing collected packages: pybullet\n",
            "Successfully installed pybullet-2.5.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjm2onHdT-Av",
        "colab_type": "text"
      },
      "source": [
        "## **IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ikr2p0Js8iB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import gym # Library for Enviornment\n",
        "import time\n",
        "import random\n",
        "import torch # PyTorch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from gym import wrappers\n",
        "import pybullet_envs \n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2nGdtlKVydr",
        "colab_type": "text"
      },
      "source": [
        "## **DEFINING EXPERIENCE REPLAY**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5rW0IDB8nTO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, max_size=1e6):\n",
        "    \"\"\"\n",
        "      ARGS:\n",
        "        max_size: maximum size of previous transitions stored in buffer\n",
        "      PARAMS:\n",
        "        storage: List containg list of transitions where each transition in list containg current state, next state , action taken to get to the next state , reward , done\n",
        "        ptr: points to cell which was enter first (oldest) among all the rest of enteries at any given time\n",
        "    \"\"\"\n",
        "    self.storage = []\n",
        "    self.max_size = max_size\n",
        "    self.ptr = 0\n",
        "\n",
        "  def add(self, transition):\n",
        "    \"\"\"\n",
        "      ARGS:\n",
        "        transition: list containg current state, next state , action taken to get to the next state , reward , done\n",
        "      ADD:\n",
        "        Adds the new transition to storage\n",
        "    \"\"\"\n",
        "    if len(self.storage) == self.max_size:\n",
        "      self.storage[int(self.ptr)] = transition\n",
        "      self.ptr = (self.ptr + 1) % self.max_size # point to the oldest cell \n",
        "    else:\n",
        "      self.storage.append(transition)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    \"\"\"\n",
        "      Returns arrays of individual columns of a transition with each array having size = batch_size\n",
        "      ARGS:\n",
        "        batch_size: selects size of transitions to sampled\n",
        "    \"\"\" \n",
        "    ind = np.random.randint(0, len(self.storage), size=batch_size)   # Samples random indices of size = batch_size\n",
        "    batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = [], [], [], [], []\n",
        "    for i in ind: \n",
        "      state, next_state, action, reward, done = self.storage[i]\n",
        "      # Append all transitions columns to list\n",
        "      batch_states.append(np.array(state, copy=False))\n",
        "      batch_next_states.append(np.array(next_state, copy=False))\n",
        "      batch_actions.append(np.array(action, copy=False))\n",
        "      batch_rewards.append(np.array(reward, copy=False))\n",
        "      batch_dones.append(np.array(done, copy=False))\n",
        "    return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1), np.array(batch_dones).reshape(-1, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jb7TTaHxWbQD",
        "colab_type": "text"
      },
      "source": [
        "## **NEURAL NETWORK FOR ACTOR MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRZvdfbg-607",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Linear_Layer(input_dim,output_dim,activation='ReLU'):\n",
        "  \"\"\"\n",
        "    ARGS:\n",
        "      input_dim: Dimensions of input \n",
        "      output_dim: Dimensions of output \n",
        "      activation: Either ReLU or Tanh will be used as activation for linear layer\n",
        "\n",
        "    RETURNS:\n",
        "      Object for linear layer\n",
        "  \"\"\"\n",
        "  if activation=='Tanh':\n",
        "    return nn.Sequential(nn.Linear(input_dim,output_dim),nn.Tanh())\n",
        "  elif activation=='ReLU':\n",
        "    return nn.Sequential(nn.Linear(input_dim,output_dim),nn.ReLU(True))\n",
        "  # If anyother activation is required it will return only linear layer\n",
        "  else:\n",
        "    return nn.Linear(input_dim,output_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CeRW4D79HL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Actor(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_action):\n",
        "    \"\"\"\n",
        "      ARGS:\n",
        "        state_dim: Dimensions of input state\n",
        "        actions_dim: Dimensions of actions (output)\n",
        "        max_actions: used to clip actions (limit range)\n",
        "    \"\"\"\n",
        "    super(Actor, self).__init__()\n",
        "    self.layer_1 = nn.Linear(state_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, action_dim)\n",
        "    self.max_action = max_action\n",
        "\n",
        "  def forward(self, x):\n",
        "    \"\"\"\n",
        "      RETURNS:\n",
        "        output of network passed through each layer\n",
        "    \"\"\"\n",
        "    x = F.relu(self.layer_1(x))\n",
        "    x = F.relu(self.layer_2(x))\n",
        "    x = self.max_action * torch.tanh(self.layer_3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRDDce8FXef7",
        "colab_type": "text"
      },
      "source": [
        "## **NEURAL NETWORK FOR TWIN CRITIC MODEL**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCee7gwR9Jrs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Critic(nn.Module):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim):\n",
        "    super(Critic, self).__init__()\n",
        "    # Defining the first Critic neural network\n",
        "    self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_2 = nn.Linear(400, 300)\n",
        "    self.layer_3 = nn.Linear(300, 1)\n",
        "    # Defining the second Critic neural network\n",
        "    self.layer_4 = nn.Linear(state_dim + action_dim, 400)\n",
        "    self.layer_5 = nn.Linear(400, 300)\n",
        "    self.layer_6 = nn.Linear(300, 1)  \n",
        "\n",
        "  def forward(self, x, u):\n",
        "    \"\"\"\n",
        "        ARGS:\n",
        "          x,u: States and Actions\n",
        "        RETURNS:\n",
        "          output of network passed through each layer\n",
        "    \"\"\"\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    # Forward-Propagation on the first Critic Neural Network\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1) # No activation \n",
        "    # Forward-Propagation on the second Critic Neural Network\n",
        "    x2 = F.relu(self.layer_4(xu))\n",
        "    x2 = F.relu(self.layer_5(x2))\n",
        "    x2 = self.layer_6(x2) # No activation \n",
        "    return x1, x2\n",
        "\n",
        "  def Q1(self, x, u):\n",
        "    \"\"\"\n",
        "      ARGS:\n",
        "          x,u: States and Actions\n",
        "      RETURNS:\n",
        "        Calculated Q1 Value for current input\n",
        "    \"\"\"\n",
        "    xu = torch.cat([x, u], 1)\n",
        "    x1 = F.relu(self.layer_1(xu))\n",
        "    x1 = F.relu(self.layer_2(x1))\n",
        "    x1 = self.layer_3(x1)\n",
        "    return x1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzIDuONodenW",
        "colab_type": "text"
      },
      "source": [
        "## **POLICY CLASS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8gAUMYvBVx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zzd0H1xukdKe",
        "colab": {}
      },
      "source": [
        "# Selecting the device (CPU or GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class TD3(object):\n",
        "  \n",
        "  def __init__(self, state_dim, action_dim, max_actions):\n",
        "    \"\"\"\n",
        "      Initialize the complete model with actor model, actor target, critic model and critic target.\n",
        "      ARGS:\n",
        "        state_dim: Dimensions of input state \n",
        "        actions_dim: Dimensions of actions (output)\n",
        "        max_actions: used to clip actions (limit range)\n",
        "    \"\"\"\n",
        "    self.actor=Actor(state_dim,action_dim,max_actions).to(device) # Object for Actor model\n",
        "    self.actor_target=Actor(state_dim,action_dim,max_actions).to(device) # Object for Actor Target model\n",
        "    self.actor_target.load_state_dict(self.actor.state_dict())  # Intialize weights of actor target with weights of actor model\n",
        "    self.actor_optimizer=torch.optim.Adam(self.actor.parameters())  # Adam Optimizer\n",
        "    self.critic=Critic(state_dim,action_dim).to(device)  # Object for Critic Model\n",
        "    self.critic_target=Critic(state_dim,action_dim).to(device) # Object for Critic Target Model\n",
        "    self.critic_target.load_state_dict(self.critic.state_dict())  # Intialize weights of critic target with weights of critic model\n",
        "    self.critic_optimizer=torch.optim.Adam(self.critic.parameters())   # Adam Optimizer\n",
        "    self.max_actions=max_actions\n",
        "    self.max_action=max_actions\n",
        "  def select_action(self, state):\n",
        "    \"\"\"\n",
        "      RETURNS:\n",
        "        action as output from actor model\n",
        "    \"\"\"\n",
        "    state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
        "    return self.actor(state).cpu().data.numpy().flatten()\n",
        "\n",
        "  def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5, policy_freq=2):\n",
        "    \n",
        "    \"\"\"\"\n",
        "      Trainer Function\n",
        "      ARGS:\n",
        "        replay_buffer: History of Previous transitions\n",
        "        epochs: Number of iterations\n",
        "        batch_size: Batch Size for training\n",
        "        discount: discount factor to reduce value of later rewards\n",
        "        tau: factor used in Polyvak Averaging for actor target model\n",
        "        policy_noise: Noise in Policy\n",
        "        noise_clip: Maximum noise value (clipped)\n",
        "        policy_freq: frequency of updating weights for policy (actor target)\n",
        "    \"\"\"\n",
        "    for it in range(iterations):\n",
        "      \n",
        "      # Convert Training data into torch tensors\n",
        "      batch_states, batch_next_states, batch_actions, batch_rewards, batch_dones = replay_buffer.sample(batch_size)\n",
        "      state = torch.Tensor(batch_states).to(device)\n",
        "      next_state = torch.Tensor(batch_next_states).to(device)\n",
        "      action = torch.Tensor(batch_actions).to(device)\n",
        "      reward = torch.Tensor(batch_rewards).to(device)\n",
        "      done = torch.Tensor(batch_dones).to(device)\n",
        "      \n",
        "      # Predict Next action using actor model\n",
        "      next_action = self.actor_target(next_state)\n",
        "      \n",
        "      \n",
        "      noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)  # Get Noise of size of sampled data\n",
        "      noise = noise.clamp(-noise_clip, noise_clip)  # Clip the noise\n",
        "      next_action = (next_action + noise).clamp(-self.max_action, self.max_action)  # Add noise to next actions\n",
        "      \n",
        "      # The two Critic targets take each the couple (s’, a’) as input and return two Q-values Qt1(s’,a’) and Qt2(s’,a’) as outputs\n",
        "      target_Q1, target_Q2 = self.critic_target(next_state, next_action)\n",
        "      \n",
        "      #Take minimum of Q values predicted by the twin critic models\n",
        "      target_Q = torch.min(target_Q1, target_Q2)\n",
        "      \n",
        "      # Final target of the two Critic models, which is: Qt = r + γ * min(Qt1, Qt2), where γ is the discount factor\n",
        "      target_Q = reward + ((1 - done) * discount * target_Q).detach()\n",
        "      \n",
        "      \n",
        "      current_Q1, current_Q2 = self.critic(state, action) # RL ,detach from graph\n",
        "      \n",
        "      \n",
        "      critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q) # Get MSE loss\n",
        "      \n",
        "      # Backpropagate\n",
        "      self.critic_optimizer.zero_grad()\n",
        "      critic_loss.backward()\n",
        "      self.critic_optimizer.step()\n",
        "      \n",
        "      if it % policy_freq == 0:\n",
        "        # Apply polvak average with policy frequency\n",
        "        # BackProp\n",
        "        actor_loss = -self.critic.Q1(state, self.actor(state)).mean()\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "        \n",
        "        # Polyak Averaging for actor\n",
        "        for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "        \n",
        "        # Polyak Averaging for critic\n",
        "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
        "          target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
        "  \n",
        "  # Making a save method to save a trained model\n",
        "  def save(self, filename, directory):\n",
        "    \"\"\"\n",
        "      Save Actor and critic models in directory\n",
        "      ARGS:\n",
        "        filename:Name of weights\n",
        "        directory:Directory for saving\n",
        "    \"\"\"\n",
        "    torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
        "    torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
        "  \n",
        "  # Making a load method to load a pre-trained model\n",
        "  def load(self, filename, directory):\n",
        "    \"\"\"\n",
        "      Save Actor and critic models in directory\n",
        "      ARGS:\n",
        "        filename:Name of weights\n",
        "        directory:Directory for saving\n",
        "    \"\"\"\n",
        "    self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
        "    self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qabqiYdp9wDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_policy(policy, eval_episodes=10):\n",
        "  \"\"\"\n",
        "    Calculates average reward over eval episode\n",
        "  \"\"\"\n",
        "  avg_reward = 0.\n",
        "  for _ in range(eval_episodes):\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    # Simulate\n",
        "    while not done:\n",
        "      action = policy.select_action(np.array(obs))\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      avg_reward += reward\n",
        "  avg_reward /= eval_episodes\n",
        "  print(8*'-','\\n','Average Reward: {}'.format(avg_reward),' \\n',8*'-','\\n')\n",
        "  return avg_reward"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGuKmH_ijf7U",
        "colab_type": "text"
      },
      "source": [
        "## **SET PARAMETERS**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFj6wbAo97lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env_name = \"AntBulletEnv-v0\" # Name of a environment (set it to any Continous environment you want)\n",
        "seed = 0 # Random seed number\n",
        "start_timesteps = 1e4 # Number of iterations/timesteps before which the model randomly chooses an action, and after which it starts to use the policy network\n",
        "eval_freq = 5e3 # How often the evaluation step is performed (after how many timesteps)\n",
        "max_timesteps = 5e5 # Total number of iterations/timesteps\n",
        "save_models = True # Boolean checker whether or not to save the pre-trained model\n",
        "expl_noise = 0.1 # Exploration noise - STD value of exploration Gaussian noise\n",
        "batch_size = 100 # Size of the batch\n",
        "discount = 0.99 # Discount factor gamma, used in the calculation of the total discounted reward\n",
        "tau = 0.005 # Target network update rate\n",
        "policy_noise = 0.2 # STD of Gaussian noise added to the actions for the exploration purposes\n",
        "noise_clip = 0.5 # Maximum value of the Gaussian noise added to the actions (policy)\n",
        "policy_freq = 2 # Number of iterations to wait before the policy network (Actor model) is updated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hjwf2HCol3XP",
        "colab_type": "text"
      },
      "source": [
        "## We create a file name for the two saved models: the Actor and Critic models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1fyH8N5z-o3o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "14c3c987-9550-446c-9554-e61d063a8081"
      },
      "source": [
        "file_name = \"%s_%s_%s\" % (\"TD3\", env_name, str(seed))\n",
        "print (\"---------------------------------------\")\n",
        "print (\"Settings: %s\" % (file_name))\n",
        "print (\"---------------------------------------\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Settings: TD3_AntBulletEnv-v0_0\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTL9uMd0ru03",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path\n",
        "work_dir = mkdir('exp', 'brs')\n",
        "monitor_dir = mkdir(work_dir, 'monitor')\n",
        "max_episode_steps = env._max_episode_steps\n",
        "save_env_vid = False\n",
        "if save_env_vid:\n",
        "  env = wrappers.Monitor(env, monitor_dir, force = True)\n",
        "  env.reset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Src07lvY-zXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MAKE DIRECTORY FOR WEIGHTS/RESULTS\n",
        "mkdir('./','results')\n",
        "mkdir('./','pytorch_models')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEAzOd47mv1Z",
        "colab_type": "text"
      },
      "source": [
        "### SETTING THE ENVIORNMENT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyQXJUIs-6BV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "85681385-f299-4abc-95c6-ba3db993135e"
      },
      "source": [
        "env = gym.make(env_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
            "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3RufYec_ADj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = float(env.action_space.high[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWEgDAQxnbem",
        "colab_type": "text"
      },
      "source": [
        "### CREATING THE MODEL AND EXPERIENCE REPLAY\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhC_5XJ__Orp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "2a191359-c5e3-45de-b509-c352d33917c0"
      },
      "source": [
        "policy = TD3(state_dim, action_dim, max_action)\n",
        "replay_buffer = ReplayBuffer()\n",
        "evaluations = [evaluate_policy(policy)]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 9.804045\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31n5eb03p-Fm",
        "colab_type": "text"
      },
      "source": [
        "### SETTING STARTING VARIABLES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vN5EvxK_QhT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_timesteps = 0\n",
        "timesteps_since_eval = 0\n",
        "episode_num = 0\n",
        "done = True\n",
        "t0 = time.time()\n",
        "avg_reward_list=[]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9gsjvtPqLgT",
        "colab_type": "text"
      },
      "source": [
        "## **TRAINING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_ouY4NH_Y0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f16971a4-6a60-441f-b114-443d5983be1f"
      },
      "source": [
        "# TRAINING\n",
        "while total_timesteps < max_timesteps:\n",
        "  \n",
        "  # If the episode is done\n",
        "  if done:\n",
        "\n",
        "    # Start the training process of the model\n",
        "    if total_timesteps != 0:\n",
        "      print(\"Total Timesteps: {} Episode Num: {} Reward: {}\".format(total_timesteps, episode_num, episode_reward))\n",
        "      policy.train(replay_buffer, episode_timesteps, batch_size, discount, tau, policy_noise, noise_clip, policy_freq)\n",
        "\n",
        "    # Evaluate POLICY of model\n",
        "    if timesteps_since_eval >= eval_freq:\n",
        "      timesteps_since_eval %= eval_freq\n",
        "      temp=evaluate_policy(policy)\n",
        "      avg_reward_list.append(temp)\n",
        "      evaluations.append(temp)\n",
        "      policy.save(file_name, directory=\"./pytorch_models\")\n",
        "      np.save(\"./results/%s\" % (file_name), evaluations)\n",
        "    \n",
        "    # Reset the state of the environment\n",
        "    obs = env.reset()\n",
        "    \n",
        "    # Set the Done to False\n",
        "    done = False\n",
        "    \n",
        "    # Set rewards and episode timesteps to zero\n",
        "    episode_reward = 0\n",
        "    episode_timesteps = 0\n",
        "    episode_num += 1\n",
        "  \n",
        "  # Before 10000 timesteps, we play random actions\n",
        "  if total_timesteps < start_timesteps:\n",
        "    action = env.action_space.sample()\n",
        "  else: # After 10000 timesteps, we switch to the model\n",
        "    action = policy.select_action(np.array(obs))\n",
        "    # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
        "    if expl_noise != 0:\n",
        "      action = (action + np.random.normal(0, expl_noise, size=env.action_space.shape[0])).clip(env.action_space.low, env.action_space.high)\n",
        "  \n",
        "  # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
        "  new_obs, reward, done, _ = env.step(action)\n",
        "  \n",
        "  # Check if the episode is done\n",
        "  done_bool = 0 if episode_timesteps + 1 == env._max_episode_steps else float(done)\n",
        "  \n",
        "  # Accumulate the total reward\n",
        "  episode_reward += reward\n",
        "  \n",
        "  # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
        "  replay_buffer.add((obs, new_obs, action, reward, done_bool))\n",
        "\n",
        "  # Update the state, the episode timestep, the total timesteps, and the timesteps since the evaluation of the policy\n",
        "  obs = new_obs\n",
        "  episode_timesteps += 1\n",
        "  total_timesteps += 1\n",
        "  timesteps_since_eval += 1\n",
        "\n",
        "# Add the last policy evaluation to our list of evaluations and we save our model\n",
        "evaluations.append(evaluate_policy(policy))\n",
        "if save_models: policy.save(\"%s\" % (file_name), directory=\"./pytorch_models\")\n",
        "np.save(\"./results/%s\" % (file_name), evaluations)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total Timesteps: 1000 Episode Num: 1 Reward: 496.2954657611786\n",
            "Total Timesteps: 1172 Episode Num: 2 Reward: 79.91545047840339\n",
            "Total Timesteps: 2172 Episode Num: 3 Reward: 518.3324693367457\n",
            "Total Timesteps: 3172 Episode Num: 4 Reward: 537.836161280911\n",
            "Total Timesteps: 4172 Episode Num: 5 Reward: 510.8818134303453\n",
            "Total Timesteps: 5172 Episode Num: 6 Reward: 516.4083629260031\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 136.943563\n",
            "---------------------------------------\n",
            "Total Timesteps: 6172 Episode Num: 7 Reward: 461.26159425501857\n",
            "Total Timesteps: 7172 Episode Num: 8 Reward: 496.2660554826422\n",
            "Total Timesteps: 8172 Episode Num: 9 Reward: 486.4106359269747\n",
            "Total Timesteps: 8354 Episode Num: 10 Reward: 82.01943954108575\n",
            "Total Timesteps: 8374 Episode Num: 11 Reward: 4.667669991715229\n",
            "Total Timesteps: 8513 Episode Num: 12 Reward: 53.858602006848365\n",
            "Total Timesteps: 9167 Episode Num: 13 Reward: 305.0356408144698\n",
            "Total Timesteps: 10167 Episode Num: 14 Reward: 492.47748928722115\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 93.034875\n",
            "---------------------------------------\n",
            "Total Timesteps: 11167 Episode Num: 15 Reward: 87.76125399242096\n",
            "Total Timesteps: 12167 Episode Num: 16 Reward: 96.27920927016426\n",
            "Total Timesteps: 13167 Episode Num: 17 Reward: 86.69416124598906\n",
            "Total Timesteps: 14167 Episode Num: 18 Reward: 180.00423777045705\n",
            "Total Timesteps: 15167 Episode Num: 19 Reward: 251.20081116822195\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 206.415050\n",
            "---------------------------------------\n",
            "Total Timesteps: 16167 Episode Num: 20 Reward: 130.1459307476557\n",
            "Total Timesteps: 17167 Episode Num: 21 Reward: 194.07987985721343\n",
            "Total Timesteps: 18167 Episode Num: 22 Reward: 167.68322992054428\n",
            "Total Timesteps: 19167 Episode Num: 23 Reward: 606.9625772144807\n",
            "Total Timesteps: 20167 Episode Num: 24 Reward: 332.8838074015023\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 50.854056\n",
            "---------------------------------------\n",
            "Total Timesteps: 20259 Episode Num: 25 Reward: 40.923763181011815\n",
            "Total Timesteps: 20383 Episode Num: 26 Reward: 46.20872105468611\n",
            "Total Timesteps: 21383 Episode Num: 27 Reward: 126.77403571932375\n",
            "Total Timesteps: 22383 Episode Num: 28 Reward: 328.7411536137596\n",
            "Total Timesteps: 23383 Episode Num: 29 Reward: 325.926934034428\n",
            "Total Timesteps: 24383 Episode Num: 30 Reward: 376.4617898831547\n",
            "Total Timesteps: 25383 Episode Num: 31 Reward: 436.4103501290166\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 295.251838\n",
            "---------------------------------------\n",
            "Total Timesteps: 26383 Episode Num: 32 Reward: 405.00638952841086\n",
            "Total Timesteps: 27383 Episode Num: 33 Reward: 182.07411012960003\n",
            "Total Timesteps: 28383 Episode Num: 34 Reward: 407.7186999701887\n",
            "Total Timesteps: 29383 Episode Num: 35 Reward: 549.5177578049362\n",
            "Total Timesteps: 30383 Episode Num: 36 Reward: 370.9265282838257\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 123.890624\n",
            "---------------------------------------\n",
            "Total Timesteps: 31383 Episode Num: 37 Reward: 130.68395686715468\n",
            "Total Timesteps: 32383 Episode Num: 38 Reward: 419.36702112859507\n",
            "Total Timesteps: 33383 Episode Num: 39 Reward: 308.09225298374537\n",
            "Total Timesteps: 34383 Episode Num: 40 Reward: 339.0536381705371\n",
            "Total Timesteps: 35383 Episode Num: 41 Reward: 356.7278176441271\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 275.913516\n",
            "---------------------------------------\n",
            "Total Timesteps: 35890 Episode Num: 42 Reward: 280.9593791011672\n",
            "Total Timesteps: 36890 Episode Num: 43 Reward: 449.7474580422778\n",
            "Total Timesteps: 37890 Episode Num: 44 Reward: 256.3276038397321\n",
            "Total Timesteps: 38890 Episode Num: 45 Reward: 350.6701161176773\n",
            "Total Timesteps: 38962 Episode Num: 46 Reward: 35.331498280956936\n",
            "Total Timesteps: 39063 Episode Num: 47 Reward: 28.638668061448104\n",
            "Total Timesteps: 39108 Episode Num: 48 Reward: 18.70773390747963\n",
            "Total Timesteps: 39157 Episode Num: 49 Reward: 31.62596629116641\n",
            "Total Timesteps: 39199 Episode Num: 50 Reward: 21.546974065900883\n",
            "Total Timesteps: 39235 Episode Num: 51 Reward: 20.278045077265954\n",
            "Total Timesteps: 39771 Episode Num: 52 Reward: 85.79941960482611\n",
            "Total Timesteps: 40771 Episode Num: 53 Reward: 222.56372168740072\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 7.345528\n",
            "---------------------------------------\n",
            "Total Timesteps: 40791 Episode Num: 54 Reward: 3.403032663996788\n",
            "Total Timesteps: 40811 Episode Num: 55 Reward: 4.4732685486516015\n",
            "Total Timesteps: 40831 Episode Num: 56 Reward: 3.7155480159242495\n",
            "Total Timesteps: 40851 Episode Num: 57 Reward: 3.981476377533971\n",
            "Total Timesteps: 41851 Episode Num: 58 Reward: 302.4567656501538\n",
            "Total Timesteps: 42851 Episode Num: 59 Reward: 414.0186529483423\n",
            "Total Timesteps: 42871 Episode Num: 60 Reward: 3.3384319566874545\n",
            "Total Timesteps: 42891 Episode Num: 61 Reward: 3.568079545978211\n",
            "Total Timesteps: 42911 Episode Num: 62 Reward: 1.2048244754938708\n",
            "Total Timesteps: 42931 Episode Num: 63 Reward: 2.6749267291795173\n",
            "Total Timesteps: 42951 Episode Num: 64 Reward: 2.632781816374628\n",
            "Total Timesteps: 42971 Episode Num: 65 Reward: 2.7716780354790025\n",
            "Total Timesteps: 42991 Episode Num: 66 Reward: 1.9937941392764147\n",
            "Total Timesteps: 43011 Episode Num: 67 Reward: 1.086526507725996\n",
            "Total Timesteps: 43031 Episode Num: 68 Reward: 1.2014019164561232\n",
            "Total Timesteps: 43051 Episode Num: 69 Reward: 3.0608866854715933\n",
            "Total Timesteps: 43071 Episode Num: 70 Reward: 3.1898742957290267\n",
            "Total Timesteps: 43091 Episode Num: 71 Reward: 2.9142541650248464\n",
            "Total Timesteps: 43111 Episode Num: 72 Reward: 2.6406512967047417\n",
            "Total Timesteps: 43131 Episode Num: 73 Reward: 4.277367304355207\n",
            "Total Timesteps: 43151 Episode Num: 74 Reward: 4.8723932620286075\n",
            "Total Timesteps: 43171 Episode Num: 75 Reward: 4.818003758555103\n",
            "Total Timesteps: 44171 Episode Num: 76 Reward: 381.7628135502782\n",
            "Total Timesteps: 44191 Episode Num: 77 Reward: 0.6993601743347728\n",
            "Total Timesteps: 44212 Episode Num: 78 Reward: 2.6787642200562214\n",
            "Total Timesteps: 44233 Episode Num: 79 Reward: 3.3849121635178285\n",
            "Total Timesteps: 44254 Episode Num: 80 Reward: 2.355923390725389\n",
            "Total Timesteps: 44321 Episode Num: 81 Reward: 20.07997680055471\n",
            "Total Timesteps: 45321 Episode Num: 82 Reward: 156.95765253544428\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 231.275010\n",
            "---------------------------------------\n",
            "Total Timesteps: 46321 Episode Num: 83 Reward: 403.78805109911826\n",
            "Total Timesteps: 47321 Episode Num: 84 Reward: 210.18430858235664\n",
            "Total Timesteps: 48321 Episode Num: 85 Reward: 104.56247109773652\n",
            "Total Timesteps: 49321 Episode Num: 86 Reward: 321.9053535269808\n",
            "Total Timesteps: 50321 Episode Num: 87 Reward: 272.30326395524156\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 429.271834\n",
            "---------------------------------------\n",
            "Total Timesteps: 51321 Episode Num: 88 Reward: 340.1818442757011\n",
            "Total Timesteps: 52321 Episode Num: 89 Reward: 465.40492210797765\n",
            "Total Timesteps: 53321 Episode Num: 90 Reward: 518.7873655879296\n",
            "Total Timesteps: 54321 Episode Num: 91 Reward: 499.02185563662135\n",
            "Total Timesteps: 55321 Episode Num: 92 Reward: 284.0487320066548\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 326.033571\n",
            "---------------------------------------\n",
            "Total Timesteps: 55771 Episode Num: 93 Reward: 164.66831431852145\n",
            "Total Timesteps: 56771 Episode Num: 94 Reward: 598.6731058514208\n",
            "Total Timesteps: 57771 Episode Num: 95 Reward: 217.88491361472109\n",
            "Total Timesteps: 58771 Episode Num: 96 Reward: 432.01881323029113\n",
            "Total Timesteps: 59771 Episode Num: 97 Reward: 311.0858340857459\n",
            "Total Timesteps: 60771 Episode Num: 98 Reward: 337.61751990809574\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 437.584368\n",
            "---------------------------------------\n",
            "Total Timesteps: 61771 Episode Num: 99 Reward: 602.7936698222848\n",
            "Total Timesteps: 62771 Episode Num: 100 Reward: 159.13289951455545\n",
            "Total Timesteps: 63771 Episode Num: 101 Reward: 421.0072142512511\n",
            "Total Timesteps: 64771 Episode Num: 102 Reward: 520.5775355693099\n",
            "Total Timesteps: 65771 Episode Num: 103 Reward: 459.02064199380067\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 285.881088\n",
            "---------------------------------------\n",
            "Total Timesteps: 66771 Episode Num: 104 Reward: 212.52277468463652\n",
            "Total Timesteps: 67771 Episode Num: 105 Reward: 441.20889245229154\n",
            "Total Timesteps: 68771 Episode Num: 106 Reward: 324.09012352352596\n",
            "Total Timesteps: 69771 Episode Num: 107 Reward: 667.8591928981825\n",
            "Total Timesteps: 70771 Episode Num: 108 Reward: 404.29110642090694\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 412.889724\n",
            "---------------------------------------\n",
            "Total Timesteps: 71771 Episode Num: 109 Reward: 411.7503854461219\n",
            "Total Timesteps: 72771 Episode Num: 110 Reward: 479.0750752507891\n",
            "Total Timesteps: 73771 Episode Num: 111 Reward: 297.1425460996628\n",
            "Total Timesteps: 74771 Episode Num: 112 Reward: 261.86951763479715\n",
            "Total Timesteps: 75771 Episode Num: 113 Reward: 514.153962596146\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 442.877773\n",
            "---------------------------------------\n",
            "Total Timesteps: 76771 Episode Num: 114 Reward: 432.66072467422\n",
            "Total Timesteps: 77771 Episode Num: 115 Reward: 449.86925167930485\n",
            "Total Timesteps: 78771 Episode Num: 116 Reward: 431.1212724839712\n",
            "Total Timesteps: 79771 Episode Num: 117 Reward: 357.05708809618534\n",
            "Total Timesteps: 80771 Episode Num: 118 Reward: 302.84001409297065\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 287.937274\n",
            "---------------------------------------\n",
            "Total Timesteps: 81771 Episode Num: 119 Reward: 295.604755371188\n",
            "Total Timesteps: 82771 Episode Num: 120 Reward: 345.24259459529964\n",
            "Total Timesteps: 83771 Episode Num: 121 Reward: 310.5603175497377\n",
            "Total Timesteps: 84771 Episode Num: 122 Reward: 641.1493150754724\n",
            "Total Timesteps: 85771 Episode Num: 123 Reward: 270.5317041995143\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 381.337772\n",
            "---------------------------------------\n",
            "Total Timesteps: 86771 Episode Num: 124 Reward: 219.89766845820432\n",
            "Total Timesteps: 87771 Episode Num: 125 Reward: 326.97782880825383\n",
            "Total Timesteps: 88771 Episode Num: 126 Reward: 365.3174296818258\n",
            "Total Timesteps: 89771 Episode Num: 127 Reward: 317.7590601775269\n",
            "Total Timesteps: 90771 Episode Num: 128 Reward: 154.2107519701535\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 485.343469\n",
            "---------------------------------------\n",
            "Total Timesteps: 91771 Episode Num: 129 Reward: 231.14133203204892\n",
            "Total Timesteps: 92771 Episode Num: 130 Reward: 429.2102139879802\n",
            "Total Timesteps: 93771 Episode Num: 131 Reward: 442.78732698071894\n",
            "Total Timesteps: 94771 Episode Num: 132 Reward: 510.18607127205917\n",
            "Total Timesteps: 95771 Episode Num: 133 Reward: 417.8821603926226\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 554.694858\n",
            "---------------------------------------\n",
            "Total Timesteps: 96771 Episode Num: 134 Reward: 447.97894138231703\n",
            "Total Timesteps: 97771 Episode Num: 135 Reward: 463.3253727120752\n",
            "Total Timesteps: 98771 Episode Num: 136 Reward: 431.2872871237246\n",
            "Total Timesteps: 99771 Episode Num: 137 Reward: 716.2805794313446\n",
            "Total Timesteps: 100771 Episode Num: 138 Reward: 520.3397272937934\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 439.272365\n",
            "---------------------------------------\n",
            "Total Timesteps: 101771 Episode Num: 139 Reward: 262.75720668211744\n",
            "Total Timesteps: 102771 Episode Num: 140 Reward: 515.3920344006413\n",
            "Total Timesteps: 103771 Episode Num: 141 Reward: 286.06974233226344\n",
            "Total Timesteps: 104771 Episode Num: 142 Reward: 302.0189317230553\n",
            "Total Timesteps: 105771 Episode Num: 143 Reward: 433.1762332114408\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 436.442647\n",
            "---------------------------------------\n",
            "Total Timesteps: 106771 Episode Num: 144 Reward: 436.40788574286296\n",
            "Total Timesteps: 107771 Episode Num: 145 Reward: 545.3640346654765\n",
            "Total Timesteps: 108771 Episode Num: 146 Reward: 421.39825579530543\n",
            "Total Timesteps: 109771 Episode Num: 147 Reward: 463.9983617477678\n",
            "Total Timesteps: 109993 Episode Num: 148 Reward: 122.20404775119141\n",
            "Total Timesteps: 110993 Episode Num: 149 Reward: 337.55135517973383\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 500.299033\n",
            "---------------------------------------\n",
            "Total Timesteps: 111993 Episode Num: 150 Reward: 385.6039989161118\n",
            "Total Timesteps: 112993 Episode Num: 151 Reward: 648.9140537436801\n",
            "Total Timesteps: 113993 Episode Num: 152 Reward: 639.766120768008\n",
            "Total Timesteps: 114993 Episode Num: 153 Reward: 551.6633851663631\n",
            "Total Timesteps: 115993 Episode Num: 154 Reward: 474.7704601928991\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 498.654671\n",
            "---------------------------------------\n",
            "Total Timesteps: 116993 Episode Num: 155 Reward: 509.55803562330516\n",
            "Total Timesteps: 117993 Episode Num: 156 Reward: 473.3560411082861\n",
            "Total Timesteps: 118993 Episode Num: 157 Reward: 475.9411509403451\n",
            "Total Timesteps: 119993 Episode Num: 158 Reward: 408.02601685741257\n",
            "Total Timesteps: 120993 Episode Num: 159 Reward: 249.10308842051285\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 449.426447\n",
            "---------------------------------------\n",
            "Total Timesteps: 121993 Episode Num: 160 Reward: 482.14078128977167\n",
            "Total Timesteps: 122993 Episode Num: 161 Reward: 469.03584862832605\n",
            "Total Timesteps: 123993 Episode Num: 162 Reward: 523.3850171512181\n",
            "Total Timesteps: 124993 Episode Num: 163 Reward: 301.74343526987775\n",
            "Total Timesteps: 125013 Episode Num: 164 Reward: 2.641216258782123\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 13.406705\n",
            "---------------------------------------\n",
            "Total Timesteps: 125033 Episode Num: 165 Reward: 2.1466910691793837\n",
            "Total Timesteps: 125053 Episode Num: 166 Reward: 3.6702618650565557\n",
            "Total Timesteps: 125074 Episode Num: 167 Reward: 4.466149901159497\n",
            "Total Timesteps: 125094 Episode Num: 168 Reward: 2.161025604986106\n",
            "Total Timesteps: 125114 Episode Num: 169 Reward: 2.8115211444117856\n",
            "Total Timesteps: 125192 Episode Num: 170 Reward: 26.61829373532409\n",
            "Total Timesteps: 126192 Episode Num: 171 Reward: 291.13927858200134\n",
            "Total Timesteps: 127192 Episode Num: 172 Reward: 466.5352330776616\n",
            "Total Timesteps: 128192 Episode Num: 173 Reward: 485.6796625148894\n",
            "Total Timesteps: 129192 Episode Num: 174 Reward: 362.8137314214924\n",
            "Total Timesteps: 130192 Episode Num: 175 Reward: 531.9851924513523\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 401.869254\n",
            "---------------------------------------\n",
            "Total Timesteps: 131192 Episode Num: 176 Reward: 437.90039580405664\n",
            "Total Timesteps: 132192 Episode Num: 177 Reward: 493.4379967144945\n",
            "Total Timesteps: 133192 Episode Num: 178 Reward: 234.10601956098998\n",
            "Total Timesteps: 134192 Episode Num: 179 Reward: 370.3040381819994\n",
            "Total Timesteps: 135192 Episode Num: 180 Reward: 453.0387366723726\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 507.076974\n",
            "---------------------------------------\n",
            "Total Timesteps: 136192 Episode Num: 181 Reward: 395.572109733641\n",
            "Total Timesteps: 137192 Episode Num: 182 Reward: 631.861443458489\n",
            "Total Timesteps: 138192 Episode Num: 183 Reward: 293.3557940584626\n",
            "Total Timesteps: 139192 Episode Num: 184 Reward: 392.667906408133\n",
            "Total Timesteps: 140192 Episode Num: 185 Reward: 495.3751870156229\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 503.284973\n",
            "---------------------------------------\n",
            "Total Timesteps: 141192 Episode Num: 186 Reward: 469.2140850805631\n",
            "Total Timesteps: 142192 Episode Num: 187 Reward: 496.3340241647586\n",
            "Total Timesteps: 143192 Episode Num: 188 Reward: 375.3723542733954\n",
            "Total Timesteps: 144192 Episode Num: 189 Reward: 654.6248650025843\n",
            "Total Timesteps: 145192 Episode Num: 190 Reward: 568.8544401675114\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 451.686712\n",
            "---------------------------------------\n",
            "Total Timesteps: 146192 Episode Num: 191 Reward: 680.5971883660054\n",
            "Total Timesteps: 147192 Episode Num: 192 Reward: 627.9169742689414\n",
            "Total Timesteps: 147212 Episode Num: 193 Reward: 2.3076930599879715\n",
            "Total Timesteps: 147232 Episode Num: 194 Reward: 2.879302367683192\n",
            "Total Timesteps: 148232 Episode Num: 195 Reward: 625.6069579564446\n",
            "Total Timesteps: 149232 Episode Num: 196 Reward: 570.9957343419921\n",
            "Total Timesteps: 150232 Episode Num: 197 Reward: 592.607549251413\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 437.892218\n",
            "---------------------------------------\n",
            "Total Timesteps: 151232 Episode Num: 198 Reward: 467.6537817748887\n",
            "Total Timesteps: 152232 Episode Num: 199 Reward: 549.1630916474434\n",
            "Total Timesteps: 153232 Episode Num: 200 Reward: 368.8768398834569\n",
            "Total Timesteps: 154232 Episode Num: 201 Reward: 640.5777803826044\n",
            "Total Timesteps: 155232 Episode Num: 202 Reward: 664.4799647877454\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 476.200868\n",
            "---------------------------------------\n",
            "Total Timesteps: 156232 Episode Num: 203 Reward: 470.7980548583213\n",
            "Total Timesteps: 157232 Episode Num: 204 Reward: 591.5368619889318\n",
            "Total Timesteps: 158232 Episode Num: 205 Reward: 430.85447037876287\n",
            "Total Timesteps: 159232 Episode Num: 206 Reward: 537.1114487288975\n",
            "Total Timesteps: 160232 Episode Num: 207 Reward: 543.9514075813121\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 504.772243\n",
            "---------------------------------------\n",
            "Total Timesteps: 161232 Episode Num: 208 Reward: 611.1259404949658\n",
            "Total Timesteps: 162232 Episode Num: 209 Reward: 516.821734644725\n",
            "Total Timesteps: 163232 Episode Num: 210 Reward: 538.2556653539166\n",
            "Total Timesteps: 164232 Episode Num: 211 Reward: 474.43373436983563\n",
            "Total Timesteps: 165232 Episode Num: 212 Reward: 453.37003861463995\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 539.104146\n",
            "---------------------------------------\n",
            "Total Timesteps: 166232 Episode Num: 213 Reward: 570.1075925741768\n",
            "Total Timesteps: 167232 Episode Num: 214 Reward: 493.14107222623005\n",
            "Total Timesteps: 168232 Episode Num: 215 Reward: 649.5046496848453\n",
            "Total Timesteps: 169232 Episode Num: 216 Reward: 555.0238306032709\n",
            "Total Timesteps: 170232 Episode Num: 217 Reward: 625.16738899998\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 324.799097\n",
            "---------------------------------------\n",
            "Total Timesteps: 171232 Episode Num: 218 Reward: 733.4744808140017\n",
            "Total Timesteps: 172232 Episode Num: 219 Reward: 589.8370316659241\n",
            "Total Timesteps: 172262 Episode Num: 220 Reward: 12.485493432650594\n",
            "Total Timesteps: 173262 Episode Num: 221 Reward: 380.8519601542499\n",
            "Total Timesteps: 174262 Episode Num: 222 Reward: 667.9970846514509\n",
            "Total Timesteps: 174287 Episode Num: 223 Reward: 4.454173641384498\n",
            "Total Timesteps: 174312 Episode Num: 224 Reward: 4.980321012470332\n",
            "Total Timesteps: 175312 Episode Num: 225 Reward: 630.4748262796058\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 563.797478\n",
            "---------------------------------------\n",
            "Total Timesteps: 176312 Episode Num: 226 Reward: 323.9577817758274\n",
            "Total Timesteps: 177312 Episode Num: 227 Reward: 325.8900795657611\n",
            "Total Timesteps: 178312 Episode Num: 228 Reward: 631.337668184524\n",
            "Total Timesteps: 179312 Episode Num: 229 Reward: 593.946605179001\n",
            "Total Timesteps: 180312 Episode Num: 230 Reward: 500.12464280726186\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 222.755797\n",
            "---------------------------------------\n",
            "Total Timesteps: 181312 Episode Num: 231 Reward: 193.83250995423845\n",
            "Total Timesteps: 182312 Episode Num: 232 Reward: 581.3591464302126\n",
            "Total Timesteps: 183312 Episode Num: 233 Reward: 472.06695514764436\n",
            "Total Timesteps: 184312 Episode Num: 234 Reward: 477.41446026230017\n",
            "Total Timesteps: 185312 Episode Num: 235 Reward: 521.5323264429886\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 663.697741\n",
            "---------------------------------------\n",
            "Total Timesteps: 186312 Episode Num: 236 Reward: 670.6192125327234\n",
            "Total Timesteps: 187312 Episode Num: 237 Reward: 791.4268947939227\n",
            "Total Timesteps: 188312 Episode Num: 238 Reward: 347.7974678347435\n",
            "Total Timesteps: 189312 Episode Num: 239 Reward: 469.22415847390084\n",
            "Total Timesteps: 190312 Episode Num: 240 Reward: 457.54945240281376\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 643.318311\n",
            "---------------------------------------\n",
            "Total Timesteps: 191312 Episode Num: 241 Reward: 567.679465245699\n",
            "Total Timesteps: 192312 Episode Num: 242 Reward: 623.7190155472238\n",
            "Total Timesteps: 193312 Episode Num: 243 Reward: 355.4648167328566\n",
            "Total Timesteps: 194312 Episode Num: 244 Reward: 678.0926658221821\n",
            "Total Timesteps: 195312 Episode Num: 245 Reward: 472.64504479943605\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 601.782011\n",
            "---------------------------------------\n",
            "Total Timesteps: 196312 Episode Num: 246 Reward: 734.9226216239535\n",
            "Total Timesteps: 197312 Episode Num: 247 Reward: 533.7669458892724\n",
            "Total Timesteps: 198312 Episode Num: 248 Reward: 504.766736608001\n",
            "Total Timesteps: 199312 Episode Num: 249 Reward: 521.962655678147\n",
            "Total Timesteps: 200312 Episode Num: 250 Reward: 708.3995227834068\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 695.564858\n",
            "---------------------------------------\n",
            "Total Timesteps: 201312 Episode Num: 251 Reward: 717.3463316351047\n",
            "Total Timesteps: 202312 Episode Num: 252 Reward: 509.24553458240564\n",
            "Total Timesteps: 203312 Episode Num: 253 Reward: 540.3530594716252\n",
            "Total Timesteps: 204312 Episode Num: 254 Reward: 587.5488921352678\n",
            "Total Timesteps: 205312 Episode Num: 255 Reward: 545.141902030369\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 685.287379\n",
            "---------------------------------------\n",
            "Total Timesteps: 206312 Episode Num: 256 Reward: 682.3248857127488\n",
            "Total Timesteps: 207312 Episode Num: 257 Reward: 614.1608926022196\n",
            "Total Timesteps: 208312 Episode Num: 258 Reward: 466.7530036121745\n",
            "Total Timesteps: 209312 Episode Num: 259 Reward: 341.5967870925459\n",
            "Total Timesteps: 210312 Episode Num: 260 Reward: 614.1202051375486\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 597.794658\n",
            "---------------------------------------\n",
            "Total Timesteps: 211312 Episode Num: 261 Reward: 708.9115562845169\n",
            "Total Timesteps: 212312 Episode Num: 262 Reward: 539.533427468814\n",
            "Total Timesteps: 213312 Episode Num: 263 Reward: 701.8017393690735\n",
            "Total Timesteps: 214312 Episode Num: 264 Reward: 650.3251854499448\n",
            "Total Timesteps: 215312 Episode Num: 265 Reward: 551.8068625627199\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 544.109974\n",
            "---------------------------------------\n",
            "Total Timesteps: 216312 Episode Num: 266 Reward: 705.071222254171\n",
            "Total Timesteps: 217312 Episode Num: 267 Reward: 605.3294642685547\n",
            "Total Timesteps: 218312 Episode Num: 268 Reward: 640.6820071223025\n",
            "Total Timesteps: 219312 Episode Num: 269 Reward: 400.11090282467916\n",
            "Total Timesteps: 220312 Episode Num: 270 Reward: 546.8412722231193\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 467.496611\n",
            "---------------------------------------\n",
            "Total Timesteps: 221312 Episode Num: 271 Reward: 473.0819161587027\n",
            "Total Timesteps: 222312 Episode Num: 272 Reward: 422.56390672609893\n",
            "Total Timesteps: 223312 Episode Num: 273 Reward: 741.755982847668\n",
            "Total Timesteps: 224312 Episode Num: 274 Reward: 515.5755757802548\n",
            "Total Timesteps: 225312 Episode Num: 275 Reward: 663.8621035049815\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 618.582293\n",
            "---------------------------------------\n",
            "Total Timesteps: 226312 Episode Num: 276 Reward: 683.5394377438026\n",
            "Total Timesteps: 227312 Episode Num: 277 Reward: 741.9606634211814\n",
            "Total Timesteps: 228312 Episode Num: 278 Reward: 659.4211876362906\n",
            "Total Timesteps: 229312 Episode Num: 279 Reward: 593.1344730190051\n",
            "Total Timesteps: 230312 Episode Num: 280 Reward: 649.7833680179419\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 663.505545\n",
            "---------------------------------------\n",
            "Total Timesteps: 231312 Episode Num: 281 Reward: 811.1531756638744\n",
            "Total Timesteps: 232312 Episode Num: 282 Reward: 653.4793426336012\n",
            "Total Timesteps: 233312 Episode Num: 283 Reward: 587.3183200616868\n",
            "Total Timesteps: 234312 Episode Num: 284 Reward: 556.4589098385869\n",
            "Total Timesteps: 235312 Episode Num: 285 Reward: 684.9390458184673\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 667.762782\n",
            "---------------------------------------\n",
            "Total Timesteps: 236312 Episode Num: 286 Reward: 727.0417746608446\n",
            "Total Timesteps: 237312 Episode Num: 287 Reward: 677.837420376707\n",
            "Total Timesteps: 238312 Episode Num: 288 Reward: 751.1881061814255\n",
            "Total Timesteps: 239312 Episode Num: 289 Reward: 656.8924276775092\n",
            "Total Timesteps: 240312 Episode Num: 290 Reward: 721.1770314858625\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 718.423040\n",
            "---------------------------------------\n",
            "Total Timesteps: 241312 Episode Num: 291 Reward: 479.10270893458943\n",
            "Total Timesteps: 242312 Episode Num: 292 Reward: 572.301725825384\n",
            "Total Timesteps: 243312 Episode Num: 293 Reward: 472.8990494030286\n",
            "Total Timesteps: 244312 Episode Num: 294 Reward: 715.7374012015439\n",
            "Total Timesteps: 245312 Episode Num: 295 Reward: 467.68597717899667\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 597.678705\n",
            "---------------------------------------\n",
            "Total Timesteps: 246312 Episode Num: 296 Reward: 584.4321991647225\n",
            "Total Timesteps: 247312 Episode Num: 297 Reward: 659.8533154151239\n",
            "Total Timesteps: 248312 Episode Num: 298 Reward: 641.3340849679954\n",
            "Total Timesteps: 249312 Episode Num: 299 Reward: 753.9923506652273\n",
            "Total Timesteps: 250312 Episode Num: 300 Reward: 647.6128549462067\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 594.550242\n",
            "---------------------------------------\n",
            "Total Timesteps: 251312 Episode Num: 301 Reward: 727.3173357744249\n",
            "Total Timesteps: 252312 Episode Num: 302 Reward: 564.3819941236027\n",
            "Total Timesteps: 253312 Episode Num: 303 Reward: 463.5970105072991\n",
            "Total Timesteps: 254312 Episode Num: 304 Reward: 796.373752399782\n",
            "Total Timesteps: 255312 Episode Num: 305 Reward: 481.2075403791293\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 675.293608\n",
            "---------------------------------------\n",
            "Total Timesteps: 256312 Episode Num: 306 Reward: 789.4435252499989\n",
            "Total Timesteps: 257312 Episode Num: 307 Reward: 614.0371042663423\n",
            "Total Timesteps: 258312 Episode Num: 308 Reward: 691.3297835025949\n",
            "Total Timesteps: 259312 Episode Num: 309 Reward: 665.4011271117693\n",
            "Total Timesteps: 260312 Episode Num: 310 Reward: 777.3939885374848\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 744.944503\n",
            "---------------------------------------\n",
            "Total Timesteps: 261312 Episode Num: 311 Reward: 925.0982597424031\n",
            "Total Timesteps: 262312 Episode Num: 312 Reward: 650.8215074765902\n",
            "Total Timesteps: 263312 Episode Num: 313 Reward: 745.0751315207216\n",
            "Total Timesteps: 264312 Episode Num: 314 Reward: 832.3849560314565\n",
            "Total Timesteps: 265312 Episode Num: 315 Reward: 765.1233995212475\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 720.307653\n",
            "---------------------------------------\n",
            "Total Timesteps: 266312 Episode Num: 316 Reward: 599.3181481735888\n",
            "Total Timesteps: 267312 Episode Num: 317 Reward: 722.5317296008022\n",
            "Total Timesteps: 268312 Episode Num: 318 Reward: 541.3875426972631\n",
            "Total Timesteps: 269312 Episode Num: 319 Reward: 760.1227229531723\n",
            "Total Timesteps: 270312 Episode Num: 320 Reward: 785.8465487046892\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 824.771935\n",
            "---------------------------------------\n",
            "Total Timesteps: 271312 Episode Num: 321 Reward: 813.952599801337\n",
            "Total Timesteps: 272312 Episode Num: 322 Reward: 658.3603390496605\n",
            "Total Timesteps: 273312 Episode Num: 323 Reward: 799.1962803124934\n",
            "Total Timesteps: 274312 Episode Num: 324 Reward: 759.4098184062779\n",
            "Total Timesteps: 275312 Episode Num: 325 Reward: 697.9179438204399\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 759.144210\n",
            "---------------------------------------\n",
            "Total Timesteps: 276312 Episode Num: 326 Reward: 931.8362519257396\n",
            "Total Timesteps: 277312 Episode Num: 327 Reward: 962.0532816483217\n",
            "Total Timesteps: 278312 Episode Num: 328 Reward: 618.3609327830812\n",
            "Total Timesteps: 279312 Episode Num: 329 Reward: 898.0321263793369\n",
            "Total Timesteps: 280312 Episode Num: 330 Reward: 701.1835737275509\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 819.736779\n",
            "---------------------------------------\n",
            "Total Timesteps: 281312 Episode Num: 331 Reward: 559.4444541017596\n",
            "Total Timesteps: 282312 Episode Num: 332 Reward: 855.743094414746\n",
            "Total Timesteps: 283312 Episode Num: 333 Reward: 637.8004754436715\n",
            "Total Timesteps: 284312 Episode Num: 334 Reward: 629.543228310348\n",
            "Total Timesteps: 285312 Episode Num: 335 Reward: 596.6495619298705\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 723.553960\n",
            "---------------------------------------\n",
            "Total Timesteps: 286312 Episode Num: 336 Reward: 986.8630141408197\n",
            "Total Timesteps: 287312 Episode Num: 337 Reward: 586.9724285399187\n",
            "Total Timesteps: 288312 Episode Num: 338 Reward: 1090.4627657135472\n",
            "Total Timesteps: 289312 Episode Num: 339 Reward: 962.1462458195952\n",
            "Total Timesteps: 290312 Episode Num: 340 Reward: 778.2500034239961\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 733.038755\n",
            "---------------------------------------\n",
            "Total Timesteps: 291312 Episode Num: 341 Reward: 289.615026947727\n",
            "Total Timesteps: 292312 Episode Num: 342 Reward: 655.5770551789846\n",
            "Total Timesteps: 293312 Episode Num: 343 Reward: 479.4895936791018\n",
            "Total Timesteps: 294312 Episode Num: 344 Reward: 776.2098257654119\n",
            "Total Timesteps: 295312 Episode Num: 345 Reward: 527.606643828512\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 718.373087\n",
            "---------------------------------------\n",
            "Total Timesteps: 296312 Episode Num: 346 Reward: 697.5155026729949\n",
            "Total Timesteps: 297312 Episode Num: 347 Reward: 899.3494034847158\n",
            "Total Timesteps: 298312 Episode Num: 348 Reward: 489.5664459229337\n",
            "Total Timesteps: 299312 Episode Num: 349 Reward: 976.8356258997565\n",
            "Total Timesteps: 300312 Episode Num: 350 Reward: 896.1506627550201\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 845.407795\n",
            "---------------------------------------\n",
            "Total Timesteps: 301312 Episode Num: 351 Reward: 892.7221126831543\n",
            "Total Timesteps: 302312 Episode Num: 352 Reward: 710.4973988519953\n",
            "Total Timesteps: 303312 Episode Num: 353 Reward: 872.974513861903\n",
            "Total Timesteps: 304312 Episode Num: 354 Reward: 859.0001148481431\n",
            "Total Timesteps: 305312 Episode Num: 355 Reward: 642.3463341628369\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 836.972337\n",
            "---------------------------------------\n",
            "Total Timesteps: 306312 Episode Num: 356 Reward: 968.4597871848607\n",
            "Total Timesteps: 307312 Episode Num: 357 Reward: 794.2281668263602\n",
            "Total Timesteps: 308312 Episode Num: 358 Reward: 951.6450849428846\n",
            "Total Timesteps: 309312 Episode Num: 359 Reward: 636.8591917303964\n",
            "Total Timesteps: 310312 Episode Num: 360 Reward: 630.4183759308897\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 858.760302\n",
            "---------------------------------------\n",
            "Total Timesteps: 311312 Episode Num: 361 Reward: 867.8682293469313\n",
            "Total Timesteps: 312312 Episode Num: 362 Reward: 969.7683359374003\n",
            "Total Timesteps: 313312 Episode Num: 363 Reward: 471.25930459184957\n",
            "Total Timesteps: 314312 Episode Num: 364 Reward: 882.7161884781273\n",
            "Total Timesteps: 315312 Episode Num: 365 Reward: 711.3700375289217\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 775.529785\n",
            "---------------------------------------\n",
            "Total Timesteps: 316312 Episode Num: 366 Reward: 889.8530601567772\n",
            "Total Timesteps: 317312 Episode Num: 367 Reward: 697.2420655947437\n",
            "Total Timesteps: 318312 Episode Num: 368 Reward: 1129.9860291258894\n",
            "Total Timesteps: 319312 Episode Num: 369 Reward: 1058.2880940827736\n",
            "Total Timesteps: 320312 Episode Num: 370 Reward: 901.2696523348282\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 924.730782\n",
            "---------------------------------------\n",
            "Total Timesteps: 321312 Episode Num: 371 Reward: 1126.6168050265967\n",
            "Total Timesteps: 322312 Episode Num: 372 Reward: 1139.8795137714824\n",
            "Total Timesteps: 323312 Episode Num: 373 Reward: 593.4186708773245\n",
            "Total Timesteps: 324312 Episode Num: 374 Reward: 763.9653131261076\n",
            "Total Timesteps: 325312 Episode Num: 375 Reward: 973.1354663177681\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1030.463135\n",
            "---------------------------------------\n",
            "Total Timesteps: 326312 Episode Num: 376 Reward: 924.8306239569821\n",
            "Total Timesteps: 327312 Episode Num: 377 Reward: 911.6847557471466\n",
            "Total Timesteps: 328312 Episode Num: 378 Reward: 612.7931328989569\n",
            "Total Timesteps: 329312 Episode Num: 379 Reward: 959.5009773275402\n",
            "Total Timesteps: 330312 Episode Num: 380 Reward: 1282.9858641311087\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1017.572245\n",
            "---------------------------------------\n",
            "Total Timesteps: 331312 Episode Num: 381 Reward: 952.8759101721284\n",
            "Total Timesteps: 332312 Episode Num: 382 Reward: 1160.8274715274158\n",
            "Total Timesteps: 333312 Episode Num: 383 Reward: 640.3980510461654\n",
            "Total Timesteps: 334312 Episode Num: 384 Reward: 803.4752807312875\n",
            "Total Timesteps: 335312 Episode Num: 385 Reward: 1008.5241162393291\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 828.823482\n",
            "---------------------------------------\n",
            "Total Timesteps: 336312 Episode Num: 386 Reward: 521.5646025335144\n",
            "Total Timesteps: 337312 Episode Num: 387 Reward: 779.0585830695854\n",
            "Total Timesteps: 338312 Episode Num: 388 Reward: 722.6377031948815\n",
            "Total Timesteps: 339312 Episode Num: 389 Reward: 285.7014875729578\n",
            "Total Timesteps: 340312 Episode Num: 390 Reward: 1152.046872835711\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 801.540901\n",
            "---------------------------------------\n",
            "Total Timesteps: 341312 Episode Num: 391 Reward: 1011.6325753217449\n",
            "Total Timesteps: 342312 Episode Num: 392 Reward: 878.1952285318661\n",
            "Total Timesteps: 343312 Episode Num: 393 Reward: 1166.4067160727693\n",
            "Total Timesteps: 344312 Episode Num: 394 Reward: 706.0132338975318\n",
            "Total Timesteps: 345312 Episode Num: 395 Reward: 1101.614569928109\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1215.264241\n",
            "---------------------------------------\n",
            "Total Timesteps: 346312 Episode Num: 396 Reward: 1114.6705767356004\n",
            "Total Timesteps: 347312 Episode Num: 397 Reward: 1196.3275264525553\n",
            "Total Timesteps: 348312 Episode Num: 398 Reward: 1304.772755299674\n",
            "Total Timesteps: 349312 Episode Num: 399 Reward: 567.7095174400513\n",
            "Total Timesteps: 350312 Episode Num: 400 Reward: 1330.7009285881404\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1332.799687\n",
            "---------------------------------------\n",
            "Total Timesteps: 351312 Episode Num: 401 Reward: 1354.62890908126\n",
            "Total Timesteps: 352312 Episode Num: 402 Reward: 1506.2670681081106\n",
            "Total Timesteps: 353312 Episode Num: 403 Reward: 1387.030814229578\n",
            "Total Timesteps: 354312 Episode Num: 404 Reward: 1216.0450992022272\n",
            "Total Timesteps: 355312 Episode Num: 405 Reward: 1330.9288772078744\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1200.716602\n",
            "---------------------------------------\n",
            "Total Timesteps: 356312 Episode Num: 406 Reward: 1221.9078115461416\n",
            "Total Timesteps: 357312 Episode Num: 407 Reward: 1107.6947933895099\n",
            "Total Timesteps: 358312 Episode Num: 408 Reward: 1349.2782811567704\n",
            "Total Timesteps: 359312 Episode Num: 409 Reward: 1453.8740018038943\n",
            "Total Timesteps: 360312 Episode Num: 410 Reward: 1463.821392251904\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1317.925941\n",
            "---------------------------------------\n",
            "Total Timesteps: 361312 Episode Num: 411 Reward: 1417.453896274527\n",
            "Total Timesteps: 362312 Episode Num: 412 Reward: 1517.0130734979505\n",
            "Total Timesteps: 363312 Episode Num: 413 Reward: 1327.3891871209767\n",
            "Total Timesteps: 364312 Episode Num: 414 Reward: 1492.2562297231261\n",
            "Total Timesteps: 365312 Episode Num: 415 Reward: 1486.970262785332\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1481.830670\n",
            "---------------------------------------\n",
            "Total Timesteps: 366312 Episode Num: 416 Reward: 1424.0842557354354\n",
            "Total Timesteps: 367312 Episode Num: 417 Reward: 1045.4109024478266\n",
            "Total Timesteps: 368312 Episode Num: 418 Reward: 828.8710632963672\n",
            "Total Timesteps: 369312 Episode Num: 419 Reward: 1513.1544434755806\n",
            "Total Timesteps: 370312 Episode Num: 420 Reward: 971.7810420734957\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1371.898561\n",
            "---------------------------------------\n",
            "Total Timesteps: 371312 Episode Num: 421 Reward: 1271.8085576423466\n",
            "Total Timesteps: 372312 Episode Num: 422 Reward: 1565.0792935236304\n",
            "Total Timesteps: 373312 Episode Num: 423 Reward: 1675.515178697971\n",
            "Total Timesteps: 374312 Episode Num: 424 Reward: 1499.5722612485945\n",
            "Total Timesteps: 375312 Episode Num: 425 Reward: 1643.6413940186712\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1621.944745\n",
            "---------------------------------------\n",
            "Total Timesteps: 376312 Episode Num: 426 Reward: 1669.7152512849043\n",
            "Total Timesteps: 377312 Episode Num: 427 Reward: 1477.789032774394\n",
            "Total Timesteps: 378312 Episode Num: 428 Reward: 1444.3418507669621\n",
            "Total Timesteps: 379312 Episode Num: 429 Reward: 1628.366991599631\n",
            "Total Timesteps: 380312 Episode Num: 430 Reward: 1599.5135783462931\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1288.730365\n",
            "---------------------------------------\n",
            "Total Timesteps: 381312 Episode Num: 431 Reward: 1403.0017521109476\n",
            "Total Timesteps: 382312 Episode Num: 432 Reward: 1698.3289079151327\n",
            "Total Timesteps: 383312 Episode Num: 433 Reward: 1624.690411764857\n",
            "Total Timesteps: 384312 Episode Num: 434 Reward: 1515.321433967699\n",
            "Total Timesteps: 385312 Episode Num: 435 Reward: 1655.4781445808192\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1739.647048\n",
            "---------------------------------------\n",
            "Total Timesteps: 386312 Episode Num: 436 Reward: 1659.3557454622774\n",
            "Total Timesteps: 387312 Episode Num: 437 Reward: 1785.5400781599085\n",
            "Total Timesteps: 388312 Episode Num: 438 Reward: 1604.5069770591429\n",
            "Total Timesteps: 389312 Episode Num: 439 Reward: 1613.863285881388\n",
            "Total Timesteps: 390312 Episode Num: 440 Reward: 1706.7972305727026\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1642.841039\n",
            "---------------------------------------\n",
            "Total Timesteps: 391312 Episode Num: 441 Reward: 1620.5861069337327\n",
            "Total Timesteps: 392312 Episode Num: 442 Reward: 1646.382711890446\n",
            "Total Timesteps: 393312 Episode Num: 443 Reward: 1522.0992290793072\n",
            "Total Timesteps: 394312 Episode Num: 444 Reward: 1623.5797875202452\n",
            "Total Timesteps: 395312 Episode Num: 445 Reward: 1522.8506941359046\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1764.884378\n",
            "---------------------------------------\n",
            "Total Timesteps: 396312 Episode Num: 446 Reward: 1718.7745763783932\n",
            "Total Timesteps: 397312 Episode Num: 447 Reward: 1771.5034535655886\n",
            "Total Timesteps: 398312 Episode Num: 448 Reward: 1769.897776432407\n",
            "Total Timesteps: 399312 Episode Num: 449 Reward: 1765.2993228377466\n",
            "Total Timesteps: 400312 Episode Num: 450 Reward: 1843.991573668242\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1654.980068\n",
            "---------------------------------------\n",
            "Total Timesteps: 401312 Episode Num: 451 Reward: 1676.9311285672375\n",
            "Total Timesteps: 402312 Episode Num: 452 Reward: 1270.2980816465858\n",
            "Total Timesteps: 403312 Episode Num: 453 Reward: 1571.980660391599\n",
            "Total Timesteps: 404312 Episode Num: 454 Reward: 1697.8505254334798\n",
            "Total Timesteps: 405312 Episode Num: 455 Reward: 1684.5826726638807\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1843.554707\n",
            "---------------------------------------\n",
            "Total Timesteps: 406312 Episode Num: 456 Reward: 1840.2309107570427\n",
            "Total Timesteps: 407312 Episode Num: 457 Reward: 1828.16979238494\n",
            "Total Timesteps: 408312 Episode Num: 458 Reward: 1708.9193119194035\n",
            "Total Timesteps: 409312 Episode Num: 459 Reward: 1779.0922930798806\n",
            "Total Timesteps: 410312 Episode Num: 460 Reward: 1694.8689831846018\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1755.244470\n",
            "---------------------------------------\n",
            "Total Timesteps: 411312 Episode Num: 461 Reward: 1798.0306669780753\n",
            "Total Timesteps: 412312 Episode Num: 462 Reward: 1711.632633526194\n",
            "Total Timesteps: 413312 Episode Num: 463 Reward: 1775.3551167049654\n",
            "Total Timesteps: 414312 Episode Num: 464 Reward: 1863.0786574137603\n",
            "Total Timesteps: 415312 Episode Num: 465 Reward: 1675.2681892171163\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1666.589368\n",
            "---------------------------------------\n",
            "Total Timesteps: 416312 Episode Num: 466 Reward: 1697.1176399417618\n",
            "Total Timesteps: 417312 Episode Num: 467 Reward: 1593.6087916563856\n",
            "Total Timesteps: 418312 Episode Num: 468 Reward: 1730.359576917279\n",
            "Total Timesteps: 419312 Episode Num: 469 Reward: 1811.6919012673075\n",
            "Total Timesteps: 420312 Episode Num: 470 Reward: 1691.0122548565028\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1844.821909\n",
            "---------------------------------------\n",
            "Total Timesteps: 421312 Episode Num: 471 Reward: 1826.0106048322305\n",
            "Total Timesteps: 422312 Episode Num: 472 Reward: 1778.6369655576216\n",
            "Total Timesteps: 423312 Episode Num: 473 Reward: 1817.0867016784769\n",
            "Total Timesteps: 424312 Episode Num: 474 Reward: 1791.246458463114\n",
            "Total Timesteps: 425312 Episode Num: 475 Reward: 1837.5265520130413\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1992.333729\n",
            "---------------------------------------\n",
            "Total Timesteps: 426312 Episode Num: 476 Reward: 1992.9145380642635\n",
            "Total Timesteps: 427312 Episode Num: 477 Reward: 1787.1064587345404\n",
            "Total Timesteps: 428312 Episode Num: 478 Reward: 1824.3886262563733\n",
            "Total Timesteps: 429312 Episode Num: 479 Reward: 1844.3215644223112\n",
            "Total Timesteps: 430312 Episode Num: 480 Reward: 1829.0657575423068\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1863.495496\n",
            "---------------------------------------\n",
            "Total Timesteps: 431312 Episode Num: 481 Reward: 1883.8066629723698\n",
            "Total Timesteps: 432312 Episode Num: 482 Reward: 1857.8204295219903\n",
            "Total Timesteps: 433312 Episode Num: 483 Reward: 2037.3693443569732\n",
            "Total Timesteps: 434312 Episode Num: 484 Reward: 2015.996371980431\n",
            "Total Timesteps: 435312 Episode Num: 485 Reward: 1971.1910222362599\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2094.819171\n",
            "---------------------------------------\n",
            "Total Timesteps: 436312 Episode Num: 486 Reward: 2080.8619897444296\n",
            "Total Timesteps: 437312 Episode Num: 487 Reward: 1820.6921587128832\n",
            "Total Timesteps: 438312 Episode Num: 488 Reward: 1709.363638399655\n",
            "Total Timesteps: 439312 Episode Num: 489 Reward: 1756.4655238724258\n",
            "Total Timesteps: 440312 Episode Num: 490 Reward: 2103.851362732474\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1917.826928\n",
            "---------------------------------------\n",
            "Total Timesteps: 441312 Episode Num: 491 Reward: 1929.1826079628459\n",
            "Total Timesteps: 442312 Episode Num: 492 Reward: 1810.572783590027\n",
            "Total Timesteps: 443312 Episode Num: 493 Reward: 1875.2818971027232\n",
            "Total Timesteps: 444312 Episode Num: 494 Reward: 1549.4908678652962\n",
            "Total Timesteps: 445312 Episode Num: 495 Reward: 1640.4435589020698\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2046.207885\n",
            "---------------------------------------\n",
            "Total Timesteps: 446312 Episode Num: 496 Reward: 2034.7642428794804\n",
            "Total Timesteps: 447312 Episode Num: 497 Reward: 1724.6927005759387\n",
            "Total Timesteps: 448312 Episode Num: 498 Reward: 1920.3215554822527\n",
            "Total Timesteps: 449312 Episode Num: 499 Reward: 1689.1479972360478\n",
            "Total Timesteps: 450312 Episode Num: 500 Reward: 1805.3888707884316\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2061.468719\n",
            "---------------------------------------\n",
            "Total Timesteps: 451312 Episode Num: 501 Reward: 1993.9543294866398\n",
            "Total Timesteps: 452312 Episode Num: 502 Reward: 1742.3239921000572\n",
            "Total Timesteps: 453312 Episode Num: 503 Reward: 1927.3117274309377\n",
            "Total Timesteps: 454312 Episode Num: 504 Reward: 1925.3201065908047\n",
            "Total Timesteps: 455312 Episode Num: 505 Reward: 2003.428646995665\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1907.659464\n",
            "---------------------------------------\n",
            "Total Timesteps: 456312 Episode Num: 506 Reward: 1835.1654467062845\n",
            "Total Timesteps: 457312 Episode Num: 507 Reward: 1934.3054621946105\n",
            "Total Timesteps: 458312 Episode Num: 508 Reward: 2032.5806141931823\n",
            "Total Timesteps: 459312 Episode Num: 509 Reward: 1926.303198537158\n",
            "Total Timesteps: 460312 Episode Num: 510 Reward: 1829.0383867711064\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1869.447302\n",
            "---------------------------------------\n",
            "Total Timesteps: 461312 Episode Num: 511 Reward: 1842.0152134170482\n",
            "Total Timesteps: 462312 Episode Num: 512 Reward: 1996.3689193253576\n",
            "Total Timesteps: 463312 Episode Num: 513 Reward: 1940.2558097094798\n",
            "Total Timesteps: 464312 Episode Num: 514 Reward: 2014.1629823568421\n",
            "Total Timesteps: 465312 Episode Num: 515 Reward: 2025.078547634946\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2093.195058\n",
            "---------------------------------------\n",
            "Total Timesteps: 466312 Episode Num: 516 Reward: 2080.587530520624\n",
            "Total Timesteps: 467312 Episode Num: 517 Reward: 1871.0280573357152\n",
            "Total Timesteps: 468312 Episode Num: 518 Reward: 2075.448522068101\n",
            "Total Timesteps: 469312 Episode Num: 519 Reward: 1995.9869060950386\n",
            "Total Timesteps: 470312 Episode Num: 520 Reward: 2001.127442121961\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1997.966095\n",
            "---------------------------------------\n",
            "Total Timesteps: 471312 Episode Num: 521 Reward: 1971.7788715267725\n",
            "Total Timesteps: 472312 Episode Num: 522 Reward: 1914.431361801535\n",
            "Total Timesteps: 473312 Episode Num: 523 Reward: 2039.0515225930012\n",
            "Total Timesteps: 474312 Episode Num: 524 Reward: 1895.2136521707369\n",
            "Total Timesteps: 475312 Episode Num: 525 Reward: 1950.607942178402\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2045.264398\n",
            "---------------------------------------\n",
            "Total Timesteps: 476312 Episode Num: 526 Reward: 2026.6407776549543\n",
            "Total Timesteps: 477312 Episode Num: 527 Reward: 2034.2103745201641\n",
            "Total Timesteps: 478312 Episode Num: 528 Reward: 2005.6988949907127\n",
            "Total Timesteps: 479312 Episode Num: 529 Reward: 1988.5565067822008\n",
            "Total Timesteps: 480312 Episode Num: 530 Reward: 1931.4079401436975\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2029.683569\n",
            "---------------------------------------\n",
            "Total Timesteps: 481312 Episode Num: 531 Reward: 2001.5829911020492\n",
            "Total Timesteps: 482312 Episode Num: 532 Reward: 1917.3710595574287\n",
            "Total Timesteps: 483312 Episode Num: 533 Reward: 2057.1064336409004\n",
            "Total Timesteps: 484312 Episode Num: 534 Reward: 1984.034282071978\n",
            "Total Timesteps: 485312 Episode Num: 535 Reward: 1871.9063212030412\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 1994.296657\n",
            "---------------------------------------\n",
            "Total Timesteps: 486312 Episode Num: 536 Reward: 1969.5077288276957\n",
            "Total Timesteps: 487312 Episode Num: 537 Reward: 2018.0574611600584\n",
            "Total Timesteps: 488312 Episode Num: 538 Reward: 1994.1860339321245\n",
            "Total Timesteps: 489312 Episode Num: 539 Reward: 1949.7454971428801\n",
            "Total Timesteps: 490312 Episode Num: 540 Reward: 1973.407907425636\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2037.176775\n",
            "---------------------------------------\n",
            "Total Timesteps: 491312 Episode Num: 541 Reward: 2067.5980902182855\n",
            "Total Timesteps: 492312 Episode Num: 542 Reward: 2001.630288844951\n",
            "Total Timesteps: 493312 Episode Num: 543 Reward: 2012.631599768019\n",
            "Total Timesteps: 494312 Episode Num: 544 Reward: 1972.0395156337484\n",
            "Total Timesteps: 495312 Episode Num: 545 Reward: 1944.7019255575647\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2022.761897\n",
            "---------------------------------------\n",
            "Total Timesteps: 496312 Episode Num: 546 Reward: 1994.384863487784\n",
            "Total Timesteps: 497312 Episode Num: 547 Reward: 2025.4715386332946\n",
            "Total Timesteps: 498312 Episode Num: 548 Reward: 2057.218917300303\n",
            "Total Timesteps: 499312 Episode Num: 549 Reward: 2052.816530439301\n",
            "---------------------------------------\n",
            "Average Reward over the Evaluation Step: 2052.499568\n",
            "---------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u18l-jIlEQgR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x_axis,avg_reward_list)\n",
        "for i in range(len(avg_reward_list)):\n",
        "  x_axis.append(str((i+1)*eval_freq/1000)+'k')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}